{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from SPARQLWrapper import SPARQLWrapper\n",
    "from itertools import combinations\n",
    "from networkx.algorithms.matching import max_weight_matching\n",
    "from pathlib import Path\n",
    "from rdflib import Graph\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE = \"wd:Q171076\"\n",
    "NODE_URL = \"http://www.wikidata.org/entity/Q171076\"\n",
    "NODE_FILE = f\"{NODE.split(':')[1]}.ttl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdf_graph_cleaner(rdf_graph):\n",
    "     # Produces a dataframe based on the RDFlib graph\n",
    "    query_df = pd.DataFrame(rdf_graph, columns=[\"Subject\", \"Property\", \"Value\"])\n",
    "    # Cleaning unwanted wrappings around our objects\n",
    "    query_df[\"Subject\"] = query_df[\"Subject\"].str.replace(\"rdflib.term.URIRef\", \"\")\n",
    "    query_df[\"Value\"] = query_df[\"Value\"].str.replace(\"rdflib.term.URIRef\", \"\")\n",
    "    query_df[\"Property\"] = query_df[\"Property\"].str.replace(\"rdflib.term.URIRef\", \"\")\n",
    "    discard_properties = [\"P921\", \"P2860\"]  # Properties to discard\n",
    "    discard_sub_obj = [\"/statement/\"]  #  Subjects and Objects to discard\n",
    "    query_df = query_df[~query_df.Property.str.contains('|'.join(discard_properties))]\n",
    "    query_df = query_df[~query_df.Subject.str.contains('|'.join(discard_sub_obj))]\n",
    "    query_df = query_df[~query_df.Value.str.contains('|'.join(discard_sub_obj))]\n",
    "    # Convert dataframe to a networkx graph\n",
    "    g = nx.from_pandas_edgelist(query_df, \"Subject\", \"Value\", edge_attr=\"Property\")\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(node: str):\n",
    "    rdf_graph_path = Path.cwd() / \"rdf_graphs\" / NODE_FILE\n",
    "    if rdf_graph_path.is_file():\n",
    "        rdf_graph_combined = Graph()\n",
    "        rdf_graph_combined.parse(rdf_graph_path)\n",
    "    else:\n",
    "        sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        sparql.setQuery(f\"\"\"\n",
    "                        CONSTRUCT {{ \n",
    "                        {node} ?p ?object.\n",
    "                        ?object ?p1 ?subject1.\n",
    "                        ?subject3 ?p4 ?object.\n",
    "                        {node} wdtn:P227 ?object2.\n",
    "                        ?subject5 wdtn:P227 ?object2. \n",
    "                        }} \n",
    "                        WHERE {{\n",
    "                            {node} ?p ?object.\n",
    "                            ?object ?p1 ?subject1.\n",
    "                            ?subject3 ?p4 ?object.\n",
    "                            OPTIONAL {{\n",
    "                                {node} wdtn:P227 ?object2.\n",
    "                                ?subject5 wdtn:P227 ?object2.}}\n",
    "                            FILTER(isURI(?subject1))\n",
    "                            FILTER(isURI(?object2))\n",
    "                        }}\n",
    "                    \"\"\")\n",
    "        query_result_part1 = sparql.queryAndConvert() # Produces a RDFlib Graph object\n",
    "\n",
    "        sparql.setQuery(f\"\"\"\n",
    "                        CONSTRUCT {{ \n",
    "                        ?subject ?p2 {node}.\n",
    "                        ?subject ?p5 ?subject4.\n",
    "                        ?subject2 ?p3 ?subject. \n",
    "                        }} \n",
    "                        WHERE {{\n",
    "                            ?subject ?p2 {node}.\n",
    "                            ?subject2 ?p3 ?subject.\n",
    "                            ?subject ?p5 ?subject4.\n",
    "                            FILTER(isURI(?subject4))\n",
    "                            MINUS{{?subject wdt:P31 wd:Q13442814}}\n",
    "                            MINUS{{?subject wdt:P31 wd:Q1348305}}\n",
    "                        }}\n",
    "                    \"\"\")\n",
    "        query_result_part2 = sparql.queryAndConvert() # Produces a RDFlib Graph object\n",
    "\n",
    "        rdf_graph_combined = query_result_part1\n",
    "        for trbl in query_result_part2:\n",
    "            rdf_graph_combined.add(trbl)\n",
    "\n",
    "        rdf_graph_combined.serialize(destination=rdf_graph_path)\n",
    "\n",
    "    return rdf_graph_cleaner(rdf_graph_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "q171076_graph = build_graph(NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifier heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifier(graph: object, node: str):\n",
    "    common_id = {}\n",
    "\n",
    "    for item in graph.neighbors(node):\n",
    "        property = graph[node][item][\"Property\"]\n",
    "        if property == \"http://www.wikidata.org/prop/direct-normalized/P227\": \n",
    "            for item_neighbor in graph.neighbors(item):\n",
    "                if item_neighbor == node:\n",
    "                    continue\n",
    "                else:\n",
    "                    property2 = graph[item][item_neighbor][\"Property\"]\n",
    "                    if property2 == property:\n",
    "                        common_id[property] = item_neighbor\n",
    "    return common_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identifier(q171076_graph, NODE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighborhood heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(graph: object, node: str):\n",
    "    nodes_to_skip = [\"http://www.w3.org/ns/lemon/ontolex#LexicalSense\", \"http://wikiba.se/ontology#BestRank\", \"http://wikiba.se/ontology#NormalRank\"]\n",
    "    node_dict = {}\n",
    "\n",
    "    for item in graph.neighbors(node):\n",
    "        for neighbor in graph.neighbors(item):\n",
    "            if neighbor == node or neighbor in nodes_to_skip:\n",
    "                continue\n",
    "            common_neighbors = sum(n in graph.neighbors(neighbor) for n in graph.neighbors(node))\n",
    "            node_dict[neighbor] = common_neighbors\n",
    "    return sorted(node_dict.items(), key=lambda item: item[1], reverse=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_neighbors = common_neighbors(q171076_graph, NODE_URL)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoleSim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iceberg pruning constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA = 0.9999\n",
    "ALPHA = 0.4\n",
    "BETA = 0.2\n",
    "THETA_BAR = (THETA - BETA)/(1 - BETA)\n",
    "DELTA = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort_nodes_by_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_nodes_by_degree(graph):\n",
    "    unsorted_degrees = []\n",
    "    unsorted_nodes = []\n",
    "\n",
    "    for node in list(graph.nodes):\n",
    "        unsorted_nodes.append(node)\n",
    "        unsorted_degrees.append(graph.degree(node))\n",
    "\n",
    "    sorted_node_degree_by_index = np.argsort(-np.array(unsorted_degrees), kind ='mergesort')\n",
    "    sorted_nodes_by_degree = np.array(unsorted_nodes)[sorted_node_degree_by_index]\n",
    "\n",
    "    return sorted_nodes_by_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort_neighborhood_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_neighborhood_degree(sorted_nodes, graph):\n",
    "    neighbor_degree_sorted_dict = {}\n",
    "\n",
    "    for node in sorted_nodes:\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "        neighbor_degrees = [graph.degree(n) for n in neighbors]\n",
    "        sorted_neighbor_degrees_by_index = np.argsort(neighbor_degrees, kind ='mergesort')\n",
    "        sorted_neighbors = np.array(neighbors)[sorted_neighbor_degrees_by_index]\n",
    "        sorted_neighbor_degrees = np.array(neighbor_degrees)[sorted_neighbor_degrees_by_index]\n",
    "        neighbor_degree_sorted_dict[node] = (sorted_neighbors, sorted_neighbor_degrees)\n",
    "\n",
    "    return neighbor_degree_sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_maximal_matching_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximal_matching_weight(graph, node_u, node_v, neighbor_degree_sorted_dict):\n",
    "    neighbors_u = list(neighbor_degree_sorted_dict[node_u][0])\n",
    "    neighbors_v = list(neighbor_degree_sorted_dict[node_v][0])\n",
    "    combined = [neighbors_u, neighbors_v]\n",
    "    all_possible_edges = list(itertools.product(*combined))\n",
    "    subgraph = nx.Graph()\n",
    "\n",
    "    for edge in all_possible_edges:\n",
    "        if graph.has_edge(*edge):\n",
    "            subgraph.add_edge(*edge)\n",
    "\n",
    "    weight = max_weight_matching(subgraph)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(sorted_node_list):\n",
    "    comb = combinations(sorted_node_list, 2)\n",
    "    return comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Initial Similarity Matrix using Pruned Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_similarity_matrix(comb, graph, neighbor_degree_sorted_dict):\n",
    "    pruned_by_one = 0\n",
    "    pruned_by_two = 0\n",
    "    pruned_by_three = 0\n",
    "\n",
    "    similarity_temp_graph_iceberg = nx.Graph()\n",
    "\n",
    "    # Begin the process of pruning using the Iceberg idea\n",
    "    for i in list(comb):\n",
    "        node_u = i[0]\n",
    "        node_v = i[1]\n",
    "\n",
    "        # Rule 1\n",
    "        if (THETA_BAR*graph.degree(node_u) > graph.degree(node_v)) or (graph.degree(node_v) > graph.degree(node_u)):\n",
    "            pruned_by_one += 1\n",
    "            continue\n",
    "        else :\n",
    "            # Rule 3\n",
    "            neighbor_1_degree_u = neighbor_degree_sorted_dict[node_u][1][0]\n",
    "            neighbor_1_degree_v = neighbor_degree_sorted_dict[node_v][1][0]\n",
    "            m11 = (1 - BETA) * (min(neighbor_1_degree_u, neighbor_1_degree_v) / max(neighbor_1_degree_u, neighbor_1_degree_v)) + BETA\n",
    "            if neighbor_1_degree_v <= neighbor_1_degree_u and ((graph.degree(node_v) - 1 + m11) < (THETA_BAR*graph.degree(node_u))):\n",
    "                pruned_by_three += 1\n",
    "                continue\n",
    "\n",
    "        # Rule 2\n",
    "        maximal_matching_weight = len(get_maximal_matching_weight(graph, node_u, node_v, neighbor_degree_sorted_dict))\n",
    "        if maximal_matching_weight >= THETA_BAR*graph.degree(node_u):\n",
    "            similarity_initial_value = (1 - BETA) * (maximal_matching_weight/graph.degree(node_u)) + BETA\n",
    "            similarity_temp_graph_iceberg.add_edge(node_u, node_v, weight=similarity_initial_value)\n",
    "        else:\n",
    "            pruned_by_two += 1\n",
    "\n",
    "    print(pruned_by_one, ' nodes were pruned by rule 1.')\n",
    "    print(pruned_by_two, ' nodes were pruned by rule 2.')\n",
    "    print(pruned_by_three, ' nodes were pruned by rule 3.')\n",
    "\n",
    "    return similarity_temp_graph_iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxiliary(graph, similarity_temp_graph_iceberg):\n",
    "    # Attempting a graph-based architecture for processing iterations of RoleSim. Required deep-copy of graph.\n",
    "    final_similarity_iceberg_graph = nx.Graph(similarity_temp_graph_iceberg)\n",
    "    list_of_nodes_selected = list(final_similarity_iceberg_graph.nodes)\n",
    "\n",
    "    skipped_present_edges = 0\n",
    "    for edgepair in combinations(list_of_nodes_selected, 2):\n",
    "        u = edgepair[0]\n",
    "        v = edgepair[1]\n",
    "        if final_similarity_iceberg_graph.has_edge(*edgepair):\n",
    "            skipped_present_edges+=1\n",
    "        else:\n",
    "            degree_u = graph.degree(u)\n",
    "            degree_v = graph.degree(v)\n",
    "            denominator = max(degree_u, degree_v)\n",
    "            numerator = min(degree_u, degree_v)\n",
    "            weight_of_edgepair = (ALPHA * (1 - BETA)* (numerator / denominator) ) + BETA\n",
    "            final_similarity_iceberg_graph.add_edge(*edgepair, weight=weight_of_edgepair)\n",
    "\n",
    "    return final_similarity_iceberg_graph, list_of_nodes_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All maximal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_maximal_matchings(subgraph):\n",
    "    maximal_matchings = []\n",
    "    partial_matchings = [{(u,v)} for (u,v) in subgraph.edges()]\n",
    "\n",
    "    while partial_matchings:\n",
    "        # get current partial matching\n",
    "        m = partial_matchings.pop()\n",
    "        nodes_m = set(itertools.chain(*m))\n",
    "\n",
    "        extended = False\n",
    "        for (u,v) in subgraph.edges():\n",
    "            if u not in nodes_m and v not in nodes_m:\n",
    "                extended = True\n",
    "                # copy m, extend it and add it to the list of partial matchings\n",
    "                m_extended = set(m)\n",
    "                m_extended.add((u,v))\n",
    "                partial_matchings.append(m_extended)\n",
    "\n",
    "        if not extended and m not in maximal_matchings:\n",
    "            maximal_matchings.append(m)\n",
    "\n",
    "    return maximal_matchings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_converged_graph(current_iteration, previous_iteration):\n",
    "    current_sum = current_iteration.size(weight='weight')\n",
    "    previous_sum = previous_iteration.size(weight='weight')\n",
    "    diff = np.fabs(current_sum - previous_sum)\n",
    "    if diff > (0.01 *current_iteration.number_of_edges()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get maximum weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_weight(previous_iteration, node_u, node_v, neighbor_degree_sorted_dict, list_of_nodes_selected_sorted):\n",
    "    neighbors_u = list(neighbor_degree_sorted_dict[node_u][0])\n",
    "    neighbors_v = list(neighbor_degree_sorted_dict[node_v][0])\n",
    "\n",
    "    neighbors_u = list(set(neighbors_u) & set(list_of_nodes_selected_sorted))\n",
    "    neighbors_v = list(set(neighbors_v) & set(list_of_nodes_selected_sorted))\n",
    "\n",
    "    combined = [neighbors_u, neighbors_v]\n",
    "    all_possible_edges = list(itertools.product(*combined))\n",
    "    subgraph = nx.Graph()\n",
    "    degree_u = len(neighbors_u)\n",
    "    degree_v = len(neighbors_v)\n",
    "\n",
    "    for edge in all_possible_edges:\n",
    "        if edge[0] != edge[1]:\n",
    "            node_x = edge[0]\n",
    "            node_y = edge[1]\n",
    "            subgraph.add_edge(*edge, weight = previous_iteration[node_x][node_y]['weight'])\n",
    "\n",
    "    trialing_using_nx_max_matching = True\n",
    "    if trialing_using_nx_max_matching:\n",
    "        maximal_matching_possibilities = [max_weight_matching(subgraph)]\n",
    "    else:\n",
    "        maximal_matching_possibilities = all_maximal_matchings(subgraph)\n",
    "\n",
    "    maximum_weight_of_all = 0\n",
    "    for possibility in maximal_matching_possibilities:\n",
    "        current_weight = 0\n",
    "        for edge in possibility:\n",
    "            current_weight += subgraph[edge[0]][edge[1]]['weight']\n",
    "        if current_weight >= maximum_weight_of_all:\n",
    "            maximum_weight_of_all = current_weight\n",
    "\n",
    "    if degree_u == 0 and degree_v == 0:\n",
    "        maximum_weight_value = 0\n",
    "    else:\n",
    "        maximum_weight_value = maximum_weight_of_all/max(degree_u, degree_v)\n",
    "        \n",
    "    return maximum_weight_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative RoleSim Alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolesim(iterative_rolesim_graph_initial, list_of_nodes_selected_sorted, neighbor_degree_sorted_dict):\n",
    "    '''\n",
    "    The code uses the edge weights of an undirected networkX graph to store the similarity values between nodes.\n",
    "    Thus, every node is connected to every other node in this graph, and if there are n nodes, then there are\n",
    "    combination(n, 2) edges (every possible edge)\n",
    "    '''\n",
    "    current_iteration = nx.Graph(iterative_rolesim_graph_initial)\n",
    "    previous_iteration = nx.Graph(iterative_rolesim_graph_initial)\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    initial_iteration = True\n",
    "    while initial_iteration == True or check_if_converged_graph(current_iteration, previous_iteration):\n",
    "        initial_iteration = False\n",
    "        k = k+1\n",
    "        previous_iteration = nx.Graph(current_iteration)\n",
    "        print('--------------------')\n",
    "        print('Iteration ', k)\n",
    "        print('--------------------')\n",
    "        print('Previous_iteration')\n",
    "        print(list(previous_iteration.edges(data=True))[:4])\n",
    "\n",
    "        start_time = time.time()\n",
    "        for edge in list(current_iteration.edges):\n",
    "            node_u = edge[0]\n",
    "            node_v = edge[1]\n",
    "            current_iteration[node_u][node_v]['weight'] = (1-BETA) *get_maximum_weight(previous_iteration, node_u, node_v, neighbor_degree_sorted_dict, list_of_nodes_selected_sorted) + BETA\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Current_iteration')\n",
    "        print(list(current_iteration.edges(data=True))[:4])\n",
    "        print('Elapsed time ', elapsed_time)\n",
    "        print('--------------------')\n",
    "\n",
    "    print('Ended after ', k, ' iterations')\n",
    "    print('Final iteration')\n",
    "    print(list(current_iteration.edges(data=True))[:4])\n",
    "\n",
    "    return current_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN Function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Graph Size: nodes(1152), edges(1195)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial Graph Size: nodes({len(q171076_graph.nodes())}), edges({len(q171076_graph.edges())})\")\n",
    "q171076_graph.remove_nodes_from(list(nx.isolates(q171076_graph)))\n",
    "sorted_nodes_by_degree = sort_nodes_by_degree(q171076_graph)\n",
    "#print(sorted_nodes_by_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_degree_dict = sort_neighborhood_degree(sorted_nodes_by_degree, q171076_graph)\n",
    "#print(neighbor_degree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37402  nodes were pruned by rule 1.\n",
      "418324  nodes were pruned by rule 2.\n",
      "183296  nodes were pruned by rule 3.\n"
     ]
    }
   ],
   "source": [
    "similarity_temp_graph_iceberg = initialize_similarity_matrix(permutations(sorted_nodes_by_degree), q171076_graph, neighbor_degree_dict)\n",
    "#print(similarity_temp_graph_iceberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux, nodes = auxiliary(q171076_graph, similarity_temp_graph_iceberg)\n",
    "#print(aux, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Iteration  1\n",
      "--------------------\n",
      "Previous_iteration\n",
      "[('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q42871728', {'weight': 1.0}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q28152342', {'weight': 0.36000000000000004}), ('http://www.wikidata.org/entity/Q570798', 'https://ckb.wikipedia.org/wiki/%D8%A6%DB%95%D9%88%D8%B1%D9%88%D9%88%D9%BE%D8%A7%DB%8C_%D9%86%D8%A7%D9%88%DB%95%D9%86%D8%AF%DB%8C', {'weight': 0.36000000000000004}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q20570218', {'weight': 0.36000000000000004})]\n",
      "Current_iteration\n",
      "[('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q42871728', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q28152342', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'https://ckb.wikipedia.org/wiki/%D8%A6%DB%95%D9%88%D8%B1%D9%88%D9%88%D9%BE%D8%A7%DB%8C_%D9%86%D8%A7%D9%88%DB%95%D9%86%D8%AF%DB%8C', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q20570218', {'weight': 0.2})]\n",
      "Elapsed time  38.80776500701904\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration  2\n",
      "--------------------\n",
      "Previous_iteration\n",
      "[('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q42871728', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q28152342', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'https://ckb.wikipedia.org/wiki/%D8%A6%DB%95%D9%88%D8%B1%D9%88%D9%88%D9%BE%D8%A7%DB%8C_%D9%86%D8%A7%D9%88%DB%95%D9%86%D8%AF%DB%8C', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q20570218', {'weight': 0.2})]\n",
      "Current_iteration\n",
      "[('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q42871728', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q28152342', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'https://ckb.wikipedia.org/wiki/%D8%A6%DB%95%D9%88%D8%B1%D9%88%D9%88%D9%BE%D8%A7%DB%8C_%D9%86%D8%A7%D9%88%DB%95%D9%86%D8%AF%DB%8C', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q20570218', {'weight': 0.2})]\n",
      "Elapsed time  38.783326864242554\n",
      "--------------------\n",
      "Ended after  2  iterations\n",
      "Final iteration\n",
      "[('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q42871728', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q28152342', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'https://ckb.wikipedia.org/wiki/%D8%A6%DB%95%D9%88%D8%B1%D9%88%D9%88%D9%BE%D8%A7%DB%8C_%D9%86%D8%A7%D9%88%DB%95%D9%86%D8%AF%DB%8C', {'weight': 0.2}), ('http://www.wikidata.org/entity/Q570798', 'http://www.wikidata.org/entity/Q20570218', {'weight': 0.2})]\n"
     ]
    }
   ],
   "source": [
    "r_sim = rolesim(nx.Graph(aux), sorted(nodes), neighbor_degree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q570798</td>\n",
       "      <td>http://www.wikidata.org/entity/Q42871728</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389162</th>\n",
       "      <td>https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q20562724</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389156</th>\n",
       "      <td>https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q214944</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389157</th>\n",
       "      <td>https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q91917557</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389158</th>\n",
       "      <td>https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...</td>\n",
       "      <td>https://el.wikipedia.org/wiki/%CE%9D%CE%B5%CE%...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194574</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2079450</td>\n",
       "      <td>http://www.wikidata.org/entity/Q91922134</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194573</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2079450</td>\n",
       "      <td>https://sl.wikipedia.org/wiki/Neolitik</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194572</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2079450</td>\n",
       "      <td>https://bs.wikipedia.org/wiki/Neolit</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194571</th>\n",
       "      <td>http://www.wikidata.org/entity/Q2079450</td>\n",
       "      <td>http://www.wikidata.org/entity/Q127644</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583739</th>\n",
       "      <td>http://www.wikidata.org/entity/P4306</td>\n",
       "      <td>http://vocab.getty.edu/aat/300019271</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583740 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   source  \\\n",
       "0                  http://www.wikidata.org/entity/Q570798   \n",
       "389162  https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...   \n",
       "389156  https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...   \n",
       "389157  https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...   \n",
       "389158  https://bn.wikivoyage.org/wiki/%E0%A6%AE%E0%A6...   \n",
       "...                                                   ...   \n",
       "194574            http://www.wikidata.org/entity/Q2079450   \n",
       "194573            http://www.wikidata.org/entity/Q2079450   \n",
       "194572            http://www.wikidata.org/entity/Q2079450   \n",
       "194571            http://www.wikidata.org/entity/Q2079450   \n",
       "583739               http://www.wikidata.org/entity/P4306   \n",
       "\n",
       "                                                   target  weight  \n",
       "0                http://www.wikidata.org/entity/Q42871728     0.2  \n",
       "389162           http://www.wikidata.org/entity/Q20562724     0.2  \n",
       "389156             http://www.wikidata.org/entity/Q214944     0.2  \n",
       "389157           http://www.wikidata.org/entity/Q91917557     0.2  \n",
       "389158  https://el.wikipedia.org/wiki/%CE%9D%CE%B5%CE%...     0.2  \n",
       "...                                                   ...     ...  \n",
       "194574           http://www.wikidata.org/entity/Q91922134     0.2  \n",
       "194573             https://sl.wikipedia.org/wiki/Neolitik     0.2  \n",
       "194572               https://bs.wikipedia.org/wiki/Neolit     0.2  \n",
       "194571             http://www.wikidata.org/entity/Q127644     0.2  \n",
       "583739               http://vocab.getty.edu/aat/300019271     0.2  \n",
       "\n",
       "[583740 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = nx.to_pandas_edgelist(r_sim)\n",
    "temp.sort_values(by=[\"weight\"], ascending=False).head(50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c85a1de1591ac7b8c945cae4b37db2e28368261b1df658ddd148bb1fb80f6b44"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('p4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
